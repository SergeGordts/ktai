Developing GPT / LLM based apps via prompt engineering

This project aims to gain a deeper knowledge of LLM-
PE to create a highly focused application. In this assign-
ment,the open-source BERT model is used. The application
will be prompted to perform the classification task of senti-
ment analysis.

The open-source uncased BERT model is used. It is a
smaller and faster version of the well-known BERT (Bidirec-
tional Encoder Representations from Transformers) model,
created by Google Ai. 

The BERT model will be prompted to perform the classifica-
tion task of sentiment analysis. The desired sentiment label
that needs to be generated by the LLM is either positive or
negative to establish correspondence with the labels within
our dataset (further elucidated subsequently), facilitating the
computation of accuracy and F1 scores.

Research Questions
1. How does PE impact the effectiveness and accuracy of
BERT for sentiment analysis tasks?
2. What are the strengths and weaknesses of BERT when
applying prompt engineering for sentiment analysis?

Discussion
The prompt-engineering steps (apart from the articulation
of the instruction) in this project led to no improvement in the
sentiment classification. This is due to the limitations of the
model. An MLM isn’t just a yes-or-no machine. Sometimes,
the token with the highest probability isn’t what we’re look-
ing for— it might not be negative or positive in our case.
So, we have to sift through the results afterwards, especially
with a model like a Masked Language Model like BERT.
Just tweaking the prompts via prompt engineering isn’t
enough to solve this problem. This requires post-filtering in
the case of an MLM, which isn’t accomplished by prompt
engineering alone. Plus, there’s no guarantee that the gener-
ated tokens will actually contain the sentiments we want. In
addition, the fact that there is a 512-token maximum doesn’t
give much leeway for prompt engineering.
Since both the accuracy and F1 scores are 0.6 and con-
sidering the app has a binary classification task (like posi-
tive/negative sentiment analysis) the model isn’t much bet-
ter than random guessing. This leads to the conclusion
that when using BERT for a binary sentiment classification
app, it seems more appropriate to develop this app through
fine-tuning rather than prompt tweaking. Fine-tuning helps
the model get better at understanding sentiment nuances,
making it more accurate in classifying sentiments. This
approach is more practical and reliable when using BERT.
