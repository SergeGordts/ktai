{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9469b8-93e2-4f00-beb2-a2e980b8edc8",
   "metadata": {},
   "source": [
    "## 1. Download a local copy of an opensourced LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac7281b-b6f4-4711-8e49-a5f4761f6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38d1ce-f98f-48b9-b394-77ef257369e0",
   "metadata": {},
   "source": [
    "<font size=\"2\">Prerequisites: create access token on huggingFace via profile settings and set this token in your system user variables </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5892cf8d-a808-4b82-83b6-2670baccd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be11a5c-6f4a-405f-95d1-ad46fd34330c",
   "metadata": {},
   "source": [
    "The opensource model [**BERT**](https://huggingface.co/google-bert/bert-base-uncased?text=This+review+expresses+a+%5BMASK%5D+sentiment+about+the+movie.+The+review%3A+This+movie+left+me+very+neutral) has a transformer architecture, meaning it uses self-attention mechanisms to understand the relationships between words in a sentence, regardless of their distance. It is a smaller and faster version of the well-known BERT (Bidirectional Encoder Representations from Transformers) model. It was created by Google Ai\n",
    "* BERT is pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "492e025f-880b-4f2d-9d86-a7fc6b13f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google-bert/bert-base-uncased\"\n",
    "filenames = [\"config.json\",\"flax_model.msgpack\",\"model.onnx\",\"model.safetensors\",\"pytorch_model.bin\",\"rust_model.ot\",\"tf_model.h5\",\"tokenizer.json\",\n",
    "             \"tokenizer_config.json\",\"vocab.txt\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b497aa-81da-452d-af40-6c0b82792187",
   "metadata": {},
   "source": [
    "<font size=\"2\"> Donwload the model so it runs locally </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2a5e34f-90d4-41f9-bb98-35594f36de5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gordts-De Laender\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "C:\\Users\\Gordts-De Laender\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\flax_model.msgpack\n",
      "C:\\Users\\Gordts-De Laender\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.onnx\n",
      "C:\\Users\\Gordts-De Laender\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
      "C:\\Users\\Gordts-De Laender\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\pytorch_model.bin\n",
      "C:\\Users\\Gordts-De Laender\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\rust_model.ot\n",
      "C:\\Users\\Gordts-De Laender\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tf_model.h5\n",
      "C:\\Users\\Gordts-De Laender\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer.json\n",
      "C:\\Users\\Gordts-De Laender\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer_config.json\n",
      "C:\\Users\\Gordts-De Laender\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\vocab.txt\n"
     ]
    }
   ],
   "source": [
    "for filename in filenames: \n",
    "    downloaded_model_path = hf_hub_download(\n",
    "            repo_id = model_id,\n",
    "            filename = filename,\n",
    "            token = HUGGING_FACE_API_KEY\n",
    "    )\n",
    "    print (downloaded_model_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00d26c-68a2-4897-9ac7-740fa54cdb60",
   "metadata": {},
   "source": [
    "[DistilBertForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification) is built upon the DistilBERT model and goes beyond the core DistilBERT model by adding a classification head on top of the encoder's output. This classification head takes the encoded representation of the sequence and transforms it into a probability distribution over different predefined classes. See [config.json](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/blob/main/config.json) for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "763bb321-65c1-4236-bc85-d06e2d5a0311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "model = BertForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaba797-7ec4-414f-9f7a-bdccbb39953b",
   "metadata": {},
   "source": [
    "The following will print out the label and the corresponding confidence score (using the softmax function). The confidence score can be interpreted as a measure of how certain the model is about its prediction. For more info see [TextClassificationPipeline](https://huggingface.co/docs/transformers/main_classes/pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3141a6a-75dc-48ad-b7ed-ee7dd8b5a7cb",
   "metadata": {},
   "source": [
    "## 2. Prompt engineering strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4407e674-51d1-4327-a537-4f23f0a3ec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.563468337059021,\n",
       "  'token': 3893,\n",
       "  'token_str': 'positive',\n",
       "  'sequence': 'this review expresses a positive sentiment about the movie. the review : this movie left me very neutral'},\n",
       " {'score': 0.07307648658752441,\n",
       "  'token': 2844,\n",
       "  'token_str': 'strong',\n",
       "  'sequence': 'this review expresses a strong sentiment about the movie. the review : this movie left me very neutral'},\n",
       " {'score': 0.05969291180372238,\n",
       "  'token': 2204,\n",
       "  'token_str': 'good',\n",
       "  'sequence': 'this review expresses a good sentiment about the movie. the review : this movie left me very neutral'},\n",
       " {'score': 0.0392783023416996,\n",
       "  'token': 3056,\n",
       "  'token_str': 'certain',\n",
       "  'sequence': 'this review expresses a certain sentiment about the movie. the review : this movie left me very neutral'},\n",
       " {'score': 0.03506943956017494,\n",
       "  'token': 4997,\n",
       "  'token_str': 'negative',\n",
       "  'sequence': 'this review expresses a negative sentiment about the movie. the review : this movie left me very neutral'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"This review expresses a [MASK] sentiment about the movie. The review: This movie left me very neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01b67d7a-c149-450f-b5a0-fb7d0b943803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9998030066490173}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Walking out of a disappointing movie, you might mutter: 'The movie was a total letdown. The acting was terrible, and the plot was predictable.' In the end, I felt...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a11bf019-24a9-4219-9352-b19375b0d246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.969277560710907}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"The book was well-written and thought-provoking, but the ending left me wanting more. (Neutral)\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "054d2d34-9f7e-4755-8b8e-782222ce4e01",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (3468017004.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[31], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    and the service was impeccable. Overall, the experience was...\")\u001b[0m\n\u001b[1;37m                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "classifier(\"After a wonderful dining experience, a customer would say: \"This restaurant was fantastic! The food was incredible, \n",
    "and the service was impeccable. Overall, the experience was...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9d4ef1-860c-4a8b-b386-5a3601aa84b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9920676946640015}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"After a wonderful dining experience, a customer would say: \"This restaurant was fantastic! The food was incredible, \n",
    "and the service was impeccable. Overall, the experience was...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f849f0c5-7764-4e47-b4de-7b9aca12a8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "with torch.no_grad(): #Disabling gradient calculation improves efficiency for inference tasks.\n",
    "    logits = model(**inputs).logits #The model processes the tokenized input and outputs a tensor named logits.logits represent the unnormalized scores (before applying softmax) for each potential sentiment class.\n",
    "\n",
    "predicted_class_id = logits.argmax().item() #This corresponds to the class with the highest predicted probability.\n",
    "model.config.id2label[predicted_class_id] #refers to the configuration of the pre-trained model (see config.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c393280-f724-484c-af55-2204f192dfa4",
   "metadata": {},
   "source": [
    "##### 2.1 adding instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f879e40-9027-4ab7-a4d1-0c11167013de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prompt = \"Identify a list of emotions that the writer of the following review is expressing:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86adff0c-2369-472e-b42e-a068e99acc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998377561569214}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(sentiment_prompt + \"I love this product! It’s so amazing and convenient.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a69a34-a7a4-42c8-be06-b526f5b0ebea",
   "metadata": {},
   "source": [
    "##### 2.2 zero-shot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2d196-6067-4bb4-a9f4-18a7e7248ef8",
   "metadata": {},
   "source": [
    "##### 2.3 few-shot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832696f4-9465-402f-a632-c2aee2bc9227",
   "metadata": {},
   "source": [
    "## 3. Stress-testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f61ef-0b2c-418b-879a-20de80c94895",
   "metadata": {},
   "source": [
    "##### 3.1 Inputs that should work well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855d8ad-8c76-4071-9677-d62fbfddf877",
   "metadata": {},
   "source": [
    "##### 3.2 Failure-case inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce911a25-ee81-438a-8de1-f848dcdbb00b",
   "metadata": {},
   "source": [
    "##### 3.3 Test for consistency (looping???)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e747cb-fc83-4ff9-b703-9e095ee5d06f",
   "metadata": {},
   "source": [
    "##### 3.4 Test for reliability (score, testing on the applied function to the model outputs in order to retrieve the scores,... )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
